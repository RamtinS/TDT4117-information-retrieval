{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39093417",
   "metadata": {},
   "source": [
    "# Assignment 3: Local Association Matrix \n",
    "\n",
    "**Student names**: Ramtin Forouzandehjoo Samavat <br>\n",
    "**Group number**: 30 <br>\n",
    "**Date**: 05.10.2025\n",
    "\n",
    "## Important notes\n",
    "Please read and follow these rules. Submissions that do not fulfill them may be returned.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. Submit in **.ipynb** format only.\n",
    "3. The assignment must be typed. Handwritten answers are not accepted.\n",
    "\n",
    "**Due date**: 12.10.2025 23:59\n",
    "\n",
    "### What you will do \n",
    "- Build a **local association matrix** from Cranfield collection.\n",
    "- Compute the **normalized association matrix**.\n",
    "- Use the normalized matrix to **identify neighborhood terms** for expansion for given queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready â€” just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd0b1f",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file."
   ]
  },
  {
   "cell_type": "code",
   "id": "10841b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:32:51.871389Z",
     "start_time": "2025-10-05T10:32:51.854700Z"
    }
   },
   "source": [
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "documents = parse_cranfield(CRAN_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "bbe7263b",
   "metadata": {},
   "source": [
    "## 3.1  Local association matrix\n",
    "\n",
    "For the given Cranfield document collection in cran.all.1400 construct a local association matrix to identify association clusters. Use the docs variable with the parsed file. Omit stopwords in the STOPWORDS list given below from the vocabulary. \n",
    "\n",
    "\n",
    "The correlation factors $c_{u,v}$ between any pair of terms $w_u$ and $w_v$ are defined as  \n",
    "$c_{u,v} = \\sum_{d_j \\in D} f_{u,j} \\cdot f_{v,j}$  \n",
    "\n",
    "$f_{u,j}$ is the raw term frequency of $w_u$ in document $d_j$.\n",
    "\n",
    "### Weighting variants: **scalar** and **metric**\n",
    "\n",
    "Add two alternative weighting schemes for the matrix (only the formula for assigning the matrix cell value changes):\n",
    "\n",
    "- **Metric weighting** :\n",
    "Let $w_u(n,j)$ and $w_v(m,j)$ denote the $n$-th and $m$-th occurrences of terms $w_u$ and $w_v$ in document $d_j$.  \n",
    "Define a distance function $r(w_u(n,j), w_v(m,j))$ (e.g., $r(i,k) = 1 + |i - k|$).  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "c_{u,v} = \\sum_{d_j \\in D} \\sum_n \\sum_m \\frac{1}{r(w_u(n,j), w_v(m,j))}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Scalar weighting** :\n",
    "Let $\\vec{s}_u = \\langle c_{u,x_1}, c_{u,x_2}, \\dots, c_{u,x_n} \\rangle$ be the neighborhood vector of term $w_u$, and similarly for $w_v$.  \n",
    "Then:\n",
    "\n",
    "$$\n",
    "c_{u,v} = \\frac{\\vec{s}_u \\cdot \\vec{s}_v}{|\\vec{s}_u| \\cdot |\\vec{s}_v|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "9433bbe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:32:51.882602Z",
     "start_time": "2025-10-05T10:32:51.880859Z"
    }
   },
   "source": [
    "# TODO: Construct a local association matrix for the cranfield collection. Use both weighting variants.\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:32:51.921121Z",
     "start_time": "2025-10-05T10:32:51.887393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize documents and remove stopwords.\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize(text, stopwords):\n",
    "  tokens = re.findall(r\"\\b[a-z]+\\b\", text.lower()) # Convert to lowercase and remove all non-alphabetic characters.\n",
    "  tokens = [word for word in tokens if word not in stopwords] # Only keep words that are not stopwords.\n",
    "  return tokens\n",
    "\n",
    "document_tokens = {} # Dictionary<doc_id, token> to store each token.\n",
    "for doc_id, doc in documents.items():\n",
    "  combined_text = f\"{doc['title']} {doc['abstract']}\"\n",
    "  document_tokens[doc_id] = tokenize(combined_text, STOPWORDS)\n",
    "\n",
    "# Check if tokenization is done correctly.\n",
    "for i in range(1, 2):\n",
    "  print(f\"Doc {i}: {document_tokens[i][:30]} ...\")"
   ],
   "id": "f21ad9210b79d82a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: ['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free'] ...\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:32:51.931797Z",
     "start_time": "2025-10-05T10:32:51.925095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a term vocabulary.\n",
    "\n",
    "vocabulary = set() # Will only keep unique tokens.\n",
    "for tokens in document_tokens.values():\n",
    "  vocabulary.update(tokens) # Add each unique term from each token to the vocabulary.\n",
    "\n",
    "# Turn it to a dictionary that maps terms and column index in the document matrix.\n",
    "vocabulary = {term: index for index, term in enumerate(sorted(vocabulary))}\n",
    "\n",
    "print(f\"Number of terms in the vocabulary: {len(vocabulary)}\")"
   ],
   "id": "7c8e36ce204c220e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in the vocabulary: 6928\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:32:52.003893Z",
     "start_time": "2025-10-05T10:32:51.936707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a document term frequency matrix with raw term frequency\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "number_documents = len(document_tokens)\n",
    "number_tokens = len(vocabulary)\n",
    "\n",
    "# Initialize matrix\n",
    "dtm = lil_matrix((number_documents, number_tokens)) # Creates a number_docs x number_tokens empty matrix.\n",
    "\n",
    "# Mapping from doc id to row indices in the matrix.\n",
    "doc_index = {doc_id: index for index, doc_id in enumerate(document_tokens.keys())}\n",
    "\n",
    "for doc_id, tokens in document_tokens.items():\n",
    "  row = doc_index[doc_id]\n",
    "  tf_counter = Counter(tokens) # tf for each term in the current document.\n",
    "  for term, tf in tf_counter.items():\n",
    "    col = vocabulary[term]\n",
    "    dtm[row, col] = tf # Assign raw term frequency\n",
    "\n",
    "print(\"Document-term matrix shape:\", dtm.shape)"
   ],
   "id": "e7badef3382c0f26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix shape: (1400, 6928)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:32:52.064018Z",
     "start_time": "2025-10-05T10:32:52.009708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Local Association Matrix (Raw Correlation)\n",
    "\n",
    "lam = dtm.T @ dtm # Termâ€“term local association matrix.\n",
    "\n",
    "print(\"Local Association Matrix shape:\", lam.shape)"
   ],
   "id": "1bce573a8e8abe04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Association Matrix shape: (6928, 6928)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:33:14.301075Z",
     "start_time": "2025-10-05T10:32:52.073599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Metric Weighting Local Association Matrix\n",
    "\n",
    "number_terms = len(vocabulary)\n",
    "\n",
    "metric_lam = lil_matrix((number_terms, number_terms), dtype=float) # Initialize metric weighted Matrix\n",
    "\n",
    "for tokens in document_tokens.values(): # Iterate over each document\n",
    "  for position1, term1 in enumerate(tokens): # Iterate over each token position for term 1.\n",
    "    if term1 not in vocabulary: continue # Skip tokens not in vocabulary.\n",
    "\n",
    "    for position2, term2 in enumerate(tokens): # Iterate over each token position for term 2.\n",
    "        if term2 not in vocabulary: continue\n",
    "\n",
    "        distance = 1 + abs(position1 - position2) # Compute distance-based weight\n",
    "\n",
    "        # Increment the matrix cell by inverse distance\n",
    "        metric_lam[vocabulary[term1], vocabulary[term2]] += 1 / distance\n",
    "\n",
    "print(\"Metric Weighting Local Association Matrix shape:\", metric_lam.shape)"
   ],
   "id": "68c20fcac7da872d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Weighting Local Association Matrix shape: (6928, 6928)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:37:42.616391Z",
     "start_time": "2025-10-05T10:33:14.316387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scalar Weighting Local Association Matrix\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "number_terms = len(vocabulary)\n",
    "\n",
    "lam_array = lam.toarray() # Convert sparse matrix to dense array for cosine computation.\n",
    "\n",
    "scalar_lam = np.zeros((number_terms, number_terms), dtype=float) # Initialize scalar weighted Matrix\n",
    "\n",
    "for term_index1 in range(number_terms):\n",
    "  for term_index2 in range(number_terms):\n",
    "\n",
    "    # Get the neighborhood vectors\n",
    "    neighborhood_vector1 = lam_array[term_index1]\n",
    "    neighborhood_vector2 = lam_array[term_index2]\n",
    "\n",
    "    # Compute vector norms\n",
    "    norm1 = norm(neighborhood_vector1)\n",
    "    norm2 = norm(neighborhood_vector2)\n",
    "\n",
    "    # Skip if either vector is zero to avoid division by zero.\n",
    "    if norm1 == 0 or norm2 == 0: continue\n",
    "\n",
    "    # Compute cosine similarity (scalar weighting)\n",
    "    cosine_similarity = np.dot(neighborhood_vector1, neighborhood_vector2) / (norm1 * norm2)\n",
    "\n",
    "    # Store the similarity score in the matrix.\n",
    "    scalar_lam[term_index1, term_index2] = cosine_similarity\n",
    "\n",
    "print(\"Scalar Weighting Local Association Matrix shape:\", scalar_lam.shape)"
   ],
   "id": "54db9550058c352e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar Weighting Local Association Matrix shape: (6928, 6928)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "ab55f9bc",
   "metadata": {},
   "source": [
    "## 3.2 Normalized association matrix\n",
    "\n",
    "Compute the normalized association matrix from the unnormalized matrix computed above. \n",
    "\n",
    "To normalize the matrix use the following formula: <br>\n",
    "$c'_{u,v} = \\frac{c_{u,v}}{c_{u,u} + c_{v,v} - c_{u,v}}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff1f5964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:40:37.340395Z",
     "start_time": "2025-10-05T10:40:18.309595Z"
    }
   },
   "source": [
    "#TODO: Compute the normalized association matrix \n",
    "\n",
    "# Your code here\n",
    "\n",
    "dense_lam = lam.toarray() # Convert sparse to dense matrix.\n",
    "\n",
    "number_terms = len(vocabulary)\n",
    "\n",
    "normalized_lam = np.zeros_like(dense_lam, dtype=float) # new array of zeros with the same shape as given input array.\n",
    "\n",
    "diagonal = np.diag(dense_lam) # self-correlation values c_{u,u} for each term\n",
    "\n",
    "# Compute normalized association for every term pair\n",
    "for term_index1 in range(number_terms):\n",
    "  for term_index2 in range(number_terms):\n",
    "\n",
    "    # c_{u,u} + c_{v,v} - c_{u,v}\n",
    "    denominator = diagonal[term_index1] + diagonal[term_index2] - dense_lam[term_index1, term_index2]\n",
    "\n",
    "    if denominator != 0:\n",
    "      # c_{u,v} / (c_{u,u} + c_{v,v} - c_{u,v})\n",
    "      normalized_lam[term_index1, term_index2] = dense_lam[term_index1, term_index2] / denominator\n",
    "    else:\n",
    "      normalized_lam[term_index1, term_index2] = 0.0\n",
    "\n",
    "\n",
    "print(\"Value range before normalization:\", lam.min(), lam.max())\n",
    "print(\"Value range after normalization:\", normalized_lam.min(), normalized_lam.max())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value range before normalization: 0.0 9649.0\n",
      "Value range after normalization: 0.0 1.0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "a7f6e8c0",
   "metadata": {},
   "source": [
    "## 3.3 Neighborhood terms\n",
    "\n",
    "With the help of the normalized local association matrix, identify the neighborhood terms that should be used for expansion for the following queries (queries_assignment3):\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e52e78f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:40:41.786755Z",
     "start_time": "2025-10-05T10:40:41.784031Z"
    }
   },
   "source": [
    "# Do not change this code\n",
    "queries_assignment3 = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "77a58c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T14:20:41.313950Z",
     "start_time": "2025-10-05T14:20:41.282599Z"
    }
   },
   "source": [
    "#TODO: Identify neighborhood terms for queries_assignment3\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Reverse vocabulary (index -> term) for mapping index from matrix to term.\n",
    "index_to_term = {index: term for term, index in vocabulary.items()}\n",
    "\n",
    "neighborhood_terms = {}\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "for query in queries_assignment3:\n",
    "\n",
    "  query_terms = query.split()\n",
    "\n",
    "  for query_term in query_terms:\n",
    "\n",
    "    if query_term not in vocabulary: continue  # skip unknown terms\n",
    "\n",
    "    query_index = vocabulary[query_term]\n",
    "    associations = normalized_lam[query_index, :] # row for query term\n",
    "\n",
    "    # Get top_k indices with the highest association (excluding the term itself)\n",
    "    top_indices = np.argsort(associations)[::-1]\n",
    "    top_indices = [index for index in top_indices if index != query_index][:top_k]\n",
    "\n",
    "    # Convert indices back to terms\n",
    "    top_terms = [index_to_term[index] for index in top_indices]\n",
    "\n",
    "    neighborhood_terms[query_term] = top_terms\n",
    "\n",
    "\n",
    "# Expansion terms for all queries\n",
    "print(\"Query Expansion Terms:\\n\")\n",
    "\n",
    "for query in queries_assignment3:\n",
    "\n",
    "  query_terms = query.split()\n",
    "\n",
    "  expansion_terms = []\n",
    "  for term in query_terms:\n",
    "    if term in neighborhood_terms:\n",
    "      expansion_terms.extend(neighborhood_terms[term])\n",
    "\n",
    "  expansion_terms = list(set(expansion_terms)) # Remove duplicates by using set\n",
    "\n",
    "  print(f\"Query: '{query}'\")\n",
    "  print(f\"Expansion terms: {expansion_terms}\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Expansion Terms:\n",
      "\n",
      "Query: 'gas pressure'\n",
      "Expansion terms: ['mach', 'equilibrium', 'number', 'injection', 'ideal', 'flow', 'results', 'real', 'air', 'jet']\n",
      "\n",
      "Query: 'structural aeroelastic flight high speed aircraft'\n",
      "Expansion terms: ['altitude', 'aircraft', 'range', 'effects', 'may', 'speed', 'fatigue', 'test', 'stations', 'vtol', 'high', 'low', 'characteristics', 'loads', 'entirely', 'structural', 'numbers', 'slipstream', 'speeds', 'ground', 'thermo', 'random', 'effect', 'structure', 'responses', 'piston']\n",
      "\n",
      "Query: 'heat conduction composite slabs'\n",
      "Expansion terms: ['boundary', 'variational', 'transfer', 'slabs', 'controlled', 'solid', 'shielded', 'medium', 'trail', 'input', 'slab', 'temperature', 'laminar', 'refractory', 'periodic', 'radiation', 'layer', 'melting', 'composite']\n",
      "\n",
      "Query: 'boundary layer control'\n",
      "Expansion terms: ['boundary', 'use', 'characteristics', 'longitudinal', 'transfer', 'number', 'trimmed', 'flow', 'laminar', 'wall', 'utilizing', 'layer']\n",
      "\n",
      "Query: 'compressible flow nozzle'\n",
      "Expansion terms: ['shock', 'boundary', 'stream', 'layer', 'fluid', 'gradient', 'jet', 'number', 'incompressible', 'base', 'viscosity', 'exit', 'laminar', 'pressure', 'nozzles']\n",
      "\n",
      "Query: 'combustion chamber injection'\n",
      "Expansion terms: ['traversed', 'ignition', 'interacted', 'gas', 'products', 'methane', 'porous', 'bluff', 'mass', 'helium', 'transversely', 'overswing', 'wall', 'ms', 'flame']\n",
      "\n",
      "Query: 'laminar turbulent transition'\n",
      "Expansion terms: ['boundary', 'heat', 'transfer', 'number', 'cooling', 'layers', 'roughness', 'laminar', 'reynolds', 'layer', 'turbulent']\n",
      "\n",
      "Query: 'fatigue crack growth'\n",
      "Expansion terms: ['exponential', 'gross', 'life', 'damage', 'random', 'behind', 'behaviour', 'structural', 'wagner', 'spheres']\n",
      "\n",
      "Query: 'wing tip vortices'\n",
      "Expansion terms: ['ratio', 'lift', 'body', 'trailing', 'investigation', 'downwash', 'appropriate', 'extreme', 'wings', 'radius', 'fins', 'vortex', 'source', 'behind', 'combinations']\n",
      "\n",
      "Query: 'propulsion efficiency'\n",
      "Expansion terms: ['rockets', 'interplanetary', 'status', 'reliability', 'systems', 'equivalence', 'production', 'source', 'addition', 'nacelle']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
